{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.6 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "eran     3\nroni     6\nnoa      9\niris    12\ndtype: int64\n       Name  marks\nrank1   Tom     99\nrank2  Jack     98\nrank3  nick     95\nrank4  juli     90\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "obj = pd.Series([3,6,9,12],index=['eran','roni','noa','iris'])\n",
    "print(obj)\n",
    "\n",
    "\n",
    "data = {\"Name\":[\"Tom\", \"Jack\", \"nick\", \"juli\"], \"marks\":[99, 98, 95, 90]}\n",
    "df = pd.DataFrame(data, index =[\"rank1\", \"rank2\", \"rank3\", \"rank4\"])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Index([' Green', 'Black ', ' Red ', 'White', ' Pink '], dtype='object')\nIndex(['Green', 'Black', 'Red', 'White', 'Pink'], dtype='object')\nIndex(['Green', 'Black ', 'Red ', 'White', 'Pink '], dtype='object')\nIndex([' Green', 'Black', ' Red', 'White', ' Pink'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#  Write a Pandas program to remove whitespaces, left sided whitespaces and right sided whitespaces of the string values of a given pandas series\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "color1 = pd.Index([' Green', 'Black ', ' Red ', 'White', ' Pink '])\n",
    "print(color1)\n",
    "print(color1.str.strip())\n",
    "print(color1.str.lstrip())\n",
    "print(color1.str.rstrip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  student_id              name  marks\n0         S1  Danniella Fenton    200\n1         S2      Ryder Storey    210\n2         S3      Bryce Jensen    190\n3         S4         Ed Bernal    222\n4         S5       Kwame Morin    199\n0         S4  Scarlette Fisher    201\n1         S5  Carla Williamson    200\n2         S6       Dante Morse    198\n3         S7    Kaiser William    219\n4         S8   Madeeha Preston    201\n"
     ]
    }
   ],
   "source": [
    "# Write a Pandas program to join the two given dataframes along rows and assign all data.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "student_data1 = pd.DataFrame({\n",
    "        'student_id': ['S1', 'S2', 'S3', 'S4', 'S5'],\n",
    "         'name': ['Danniella Fenton', 'Ryder Storey', 'Bryce Jensen', 'Ed Bernal', 'Kwame Morin'], \n",
    "        'marks': [200, 210, 190, 222, 199]})\n",
    "\n",
    "student_data2 = pd.DataFrame({\n",
    "        'student_id': ['S4', 'S5', 'S6', 'S7', 'S8'],\n",
    "        'name': ['Scarlette Fisher', 'Carla Williamson', 'Dante Morse', 'Kaiser William', 'Madeeha Preston'], \n",
    "        'marks': [201, 200, 198, 219, 201]})\n",
    "\n",
    "result_data = pd.concat([student_data1, student_data2])\n",
    "print(result_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Original DataFrames:\n  student_id              name  marks\n0         S1  Danniella Fenton    200\n1         S2      Ryder Storey    210\n2         S3      Bryce Jensen    190\n3         S4         Ed Bernal    222\n4         S5       Kwame Morin    199\n\nNew Row(s)\nstudent_id                  S6\nname          Scarlette Fisher\nmarks                      205\ndtype: object\n\nCombined Data:\n  student_id              name  marks\n0         S1  Danniella Fenton    200\n1         S2      Ryder Storey    210\n2         S3      Bryce Jensen    190\n3         S4         Ed Bernal    222\n4         S5       Kwame Morin    199\n5         S6  Scarlette Fisher    205\n"
     ]
    }
   ],
   "source": [
    "# Write a Pandas program to append rows to an existing DataFrame and display the combined data.\n",
    "\n",
    "import pandas as pd\n",
    "student_data1 = pd.DataFrame({\n",
    "        'student_id': ['S1', 'S2', 'S3', 'S4', 'S5'],\n",
    "         'name': ['Danniella Fenton', 'Ryder Storey', 'Bryce Jensen', 'Ed Bernal', 'Kwame Morin'], \n",
    "        'marks': [200, 210, 190, 222, 199]})\n",
    "\n",
    "s6 = pd.Series(['S6', 'Scarlette Fisher', 205], index=['student_id', 'name', 'marks'])\n",
    "combined_data = student_data1.append(s6, ignore_index = True)\n",
    "print(combined_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  student_id              name  marks\n0         S1  Danniella Fenton    200\n1         S2      Ryder Storey    210\n2         S3      Bryce Jensen    190\n3         S4         Ed Bernal    222\n4         S5       Kwame Morin    199\n0         S4  Scarlette Fisher    201\n1         S5  Carla Williamson    200\n2         S6       Dante Morse    198\n3         S7    Kaiser William    219\n4         S8   Madeeha Preston    201\n\nNow join the said result_data and df_exam_data along student_id:\n  student_id              name  marks  exam_id\n0         S1  Danniella Fenton    200       23\n1         S2      Ryder Storey    210       45\n2         S3      Bryce Jensen    190       12\n3         S4         Ed Bernal    222       67\n4         S4  Scarlette Fisher    201       67\n5         S5       Kwame Morin    199       21\n6         S5  Carla Williamson    200       21\n7         S7    Kaiser William    219       55\n8         S8   Madeeha Preston    201       33\n"
     ]
    }
   ],
   "source": [
    "# Write a Pandas program to join the two given dataframes along rows and merge with another dataframe along the common column id.\n",
    "import pandas as pd\n",
    "student_data1 = pd.DataFrame({\n",
    "        'student_id': ['S1', 'S2', 'S3', 'S4', 'S5'],\n",
    "         'name': ['Danniella Fenton', 'Ryder Storey', 'Bryce Jensen', 'Ed Bernal', 'Kwame Morin'], \n",
    "        'marks': [200, 210, 190, 222, 199]})\n",
    "\n",
    "student_data2 = pd.DataFrame({\n",
    "        'student_id': ['S4', 'S5', 'S6', 'S7', 'S8'],\n",
    "        'name': ['Scarlette Fisher', 'Carla Williamson', 'Dante Morse', 'Kaiser William', 'Madeeha Preston'], \n",
    "        'marks': [201, 200, 198, 219, 201]})\n",
    "\n",
    "exam_data = pd.DataFrame({\n",
    "        'student_id': ['S1', 'S2', 'S3', 'S4', 'S5', 'S7', 'S8', 'S9', 'S10', 'S11', 'S12', 'S13'],\n",
    "        'exam_id': [23, 45, 12, 67, 21, 55, 33, 14, 56, 83, 88, 12]})\n",
    "\n",
    "result_data = pd.concat([student_data1, student_data2])\n",
    "print(result_data)\n",
    "final_merged_data = pd.merge(result_data, exam_data, on='student_id')\n",
    "print(final_merged_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  key1 key2    P    Q    R    S\n0   K0   K0   P0   Q0  NaN  NaN\n1   K0   K1   P1   Q1  NaN  NaN\n2   K1   K0   P2   Q2  NaN  NaN\n3   K2   K1   P3   Q3  NaN  NaN\n4   K0   K0  NaN  NaN   R0   S0\n5   K1   K0  NaN  NaN   R1   S1\n6   K1   K0  NaN  NaN   R2   S2\n7   K2   K0  NaN  NaN   R3   S3\n"
     ]
    }
   ],
   "source": [
    "# Write a Pandas program to merge two given dataframes with different columns.\n",
    "\n",
    "import pandas as pd\n",
    "data1 = pd.DataFrame({'key1': ['K0', 'K0', 'K1', 'K2'],\n",
    "                     'key2': ['K0', 'K1', 'K0', 'K1'],\n",
    "                     'P': ['P0', 'P1', 'P2', 'P3'],\n",
    "                     'Q': ['Q0', 'Q1', 'Q2', 'Q3']}) \n",
    "data2 = pd.DataFrame({'key1': ['K0', 'K1', 'K1', 'K2'],\n",
    "                      'key2': ['K0', 'K0', 'K0', 'K0'],\n",
    "                      'R': ['R0', 'R1', 'R2', 'R3'],\n",
    "                      'S': ['S0', 'S1', 'S2', 'S3']})\n",
    "\n",
    "result = pd.concat([data1,data2], axis=0, ignore_index=True)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "True\nFalse\n"
     ]
    }
   ],
   "source": [
    "# Write a Python program to check that a string contains only a certain set of characters (in this case a-z, A-Z and 0-9).\n",
    "import re\n",
    "def is_allowed_specific_char(string):\n",
    "    charRe = re.compile(r'[^a-zA-Z0-9.]')\n",
    "    string = charRe.search(string)\n",
    "    return not bool(string)\n",
    "\n",
    "print(is_allowed_specific_char(\"ABCDEFabcdef123450\")) \n",
    "print(is_allowed_specific_char(\"*&%@#!}{\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1, 2, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "# Write a Python function that takes a list and returns a new list with unique elements of the first list.\n",
    "def unique_list(l):\n",
    "  x = []\n",
    "  for a in l:\n",
    "    if a not in x:\n",
    "      x.append(a)\n",
    "  return x\n",
    "\n",
    "print(unique_list([1,2,3,3,3,3,4,5])) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "yah\n\n"
     ]
    }
   ],
   "source": [
    "# Write a Python program that accepts a word from the user and reverse it.\n",
    "\n",
    "word = input(\"Input a word to reverse: \")\n",
    "\n",
    "for char in range(len(word) - 1, -1, -1):\n",
    "  print(word[char], end=\"\")\n",
    "print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1\n1\n2\n3\n5\n8\n13\n21\n34\n"
     ]
    }
   ],
   "source": [
    "# Write a Python program to get the Fibonacci series between 0 to 50\n",
    "\n",
    "x,y=0,1\n",
    "\n",
    "while y<50:\n",
    "    print(y)\n",
    "    x,y = y,x+y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a Pandas program to display the first, last name, salary and department number for those employees whose first name does not contain the letter 'M'.\n",
    "import pandas as pd\n",
    "employees = pd.read_csv(r\"EMPLOYEES.csv\")\n",
    "departments = pd.read_csv(r\"DEPARTMENTS.csv\")\n",
    "job_history = pd.read_csv(r\"JOB_HISTORY.csv\")\n",
    "jobs = pd.read_csv(r\"JOBS.csv\")\n",
    "countries = pd.read_csv(r\"COUNTRIES.csv\")\n",
    "regions = pd.read_csv(r\"REGIONS.csv\")\n",
    "locations = pd.read_csv(r\"LOCATIONS.csv\")\n",
    "print(\"Last name       First name      Salary    Department ID\")\n",
    "\n",
    "result = employees[employees['first_name'].str.find('M')==-1]\n",
    "for index, row in result.iterrows():\n",
    "    print(row['last_name'].ljust(15),row['first_name'].ljust(15),str(row['salary']).ljust(9),row['department_id'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Original DataFrame:\n   school_code class            name date_Of_Birth   age  height  weight  \\\nS1        s001     V  Alberto Franco     15/05/2002   12     173      35   \nS2        s002     V    Gino Mcneill     17/05/2002   12     192      32   \nS3        s003    VI     Ryan Parkes     16/02/1999   13     186      33   \nS4        s001    VI    Eesha Hinton     25/09/1998   13     167      30   \nS5        s002     V    Gino Mcneill     11/05/2002   14     151      31   \nS6        s004    VI    David Parkes     15/09/1997   12     159      32   \n\n    address  \nS1  street1  \nS2  street2  \nS3  street3  \nS4  street1  \nS5  street2  \nS6  street4  \n\nSplit the said data on school_code wise:\n\nGroup:\ns001\n   school_code class            name date_Of_Birth   age  height  weight  \\\nS1        s001     V  Alberto Franco     15/05/2002   12     173      35   \nS4        s001    VI    Eesha Hinton     25/09/1998   13     167      30   \n\n    address  \nS1  street1  \nS4  street1  \n\nGroup:\ns002\n   school_code class          name date_Of_Birth   age  height  weight  \\\nS2        s002     V  Gino Mcneill     17/05/2002   12     192      32   \nS5        s002     V  Gino Mcneill     11/05/2002   14     151      31   \n\n    address  \nS2  street2  \nS5  street2  \n\nGroup:\ns003\n   school_code class         name date_Of_Birth   age  height  weight  address\nS3        s003    VI  Ryan Parkes     16/02/1999   13     186      33  street3\n\nGroup:\ns004\n   school_code class          name date_Of_Birth   age  height  weight  \\\nS6        s004    VI  David Parkes     15/09/1997   12     159      32   \n\n    address  \nS6  street4  \n\nType of the object:\n<class 'pandas.core.groupby.generic.DataFrameGroupBy'>\n"
     ]
    }
   ],
   "source": [
    "# Write a Pandas program to split the following dataframe into groups based on school code. Also check the type of GroupBy object.\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', None)\n",
    "#pd.set_option('display.max_columns', None)\n",
    "student_data = pd.DataFrame({\n",
    "    'school_code': ['s001','s002','s003','s001','s002','s004'],\n",
    "    'class': ['V', 'V', 'VI', 'VI', 'V', 'VI'],\n",
    "    'name': ['Alberto Franco','Gino Mcneill','Ryan Parkes', 'Eesha Hinton', 'Gino Mcneill', 'David Parkes'],\n",
    "    'date_Of_Birth ': ['15/05/2002','17/05/2002','16/02/1999','25/09/1998','11/05/2002','15/09/1997'],\n",
    "    'age': [12, 12, 13, 13, 14, 12],\n",
    "    'height': [173, 192, 186, 167, 151, 159],\n",
    "    'weight': [35, 32, 33, 30, 31, 32],\n",
    "    'address': ['street1', 'street2', 'street3', 'street1', 'street2', 'street4']},\n",
    "    index=['S1', 'S2', 'S3', 'S4', 'S5', 'S6'])\n",
    "\n",
    "print(\"Original DataFrame:\")\n",
    "print(student_data)\n",
    "print('\\nSplit the said data on school_code wise:')\n",
    "result = student_data.groupby(['school_code'])\n",
    "for name,group in result:\n",
    "    print(\"\\nGroup:\")\n",
    "    print(name)\n",
    "    print(group)\n",
    "print(\"\\nType of the object:\")\n",
    "print(type(result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A valid UID must follow the rules below:\n",
    "# It must contain at least  uppercase English alphabet characters.\n",
    "# It must contain at least  digits ( - ).\n",
    "# It should only contain alphanumeric characters ( - ,  -  &  - ).\n",
    "# No character should repeat.\n",
    "# There must be exactly  characters in a valid UID.\n",
    "\n",
    "import re\n",
    "\n",
    "for _ in range(int(input())):\n",
    "    u = ''.join(sorted(input()))\n",
    "    try:\n",
    "        assert re.search(r'[A-Z]{2}', u)\n",
    "        assert re.search(r'\\d\\d\\d', u)\n",
    "        assert not re.search(r'[^a-zA-Z0-9]', u)\n",
    "        assert not re.search(r'(.)\\1', u)\n",
    "        assert len(u) == 10\n",
    "    except:\n",
    "        print('Invalid')\n",
    "    else:\n",
    "        print('Valid')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'fdvf'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-586722d3f2f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0memails\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: 'fdvf'"
     ]
    }
   ],
   "source": [
    "# Valid email addresses must follow these rules:\n",
    "# It must have the username@websitename.extension format type.\n",
    "# The username can only contain letters, digits, dashes and underscores.\n",
    "# The website name can only have letters and digits.\n",
    "# The maximum length of the extension is .\n",
    "\n",
    "import re\n",
    "\n",
    "def fun(s):\n",
    "  pattern = re.compile(\"^[\\\\w-]+@[0-9a-zA-Z]+\\\\.[a-z]{1,3}$\")\n",
    "  return pattern.match(s)\n",
    "       \n",
    "def filter_mail(emails):\n",
    "    return list(filter(fun, emails))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    n = int(input())\n",
    "    emails = []\n",
    "    for _ in range(n):\n",
    "        emails.append(input())\n",
    "\n",
    "filtered_emails = filter_mail(emails)\n",
    "filtered_emails.sort()\n",
    "print(filtered_emails)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{1: {'LTV_AVG': 8.956521739130435, 'LTV_ITEMS': 115}, 7: {'LTV_AVG': 4.5, 'LTV_ITEMS': 2}, 2: {'LTV_AVG': 203.33333333333334, 'LTV_ITEMS': 3}, 3: {'LTV_AVG': 5.0, 'LTV_ITEMS': 12}, 77: {'LTV_AVG': 5.0, 'LTV_ITEMS': 12}, 23: {'LTV_AVG': 2.0, 'LTV_ITEMS': 1}}\n"
     ]
    }
   ],
   "source": [
    "# optimove question :\n",
    "# We Have a dictionary with LTV data per user “historical_data”.\n",
    "\n",
    "# Every few minutes we are receiving a smaller list with activities of new/existing users “new_data”.\n",
    "# Complete the function update_hist that recieves a list of events as an input and updates the historical_data dictionary\n",
    "\n",
    "\n",
    "historical_data = {1: {\"LTV_AVG\": 10, \"LTV_ITEMS\": 100}, 7: {\"LTV_AVG\": 4.5, \"LTV_ITEMS\": 2}}\n",
    "\n",
    "new_data = [{\"USER_ID\": 1, \"amount\": 30, \"items\": 15}, {\"USER_ID\": 2, \"amount\": 10, \"items\": 1},\n",
    "            {\"USER_ID\": 3, \"amount\": 60, \"items\": 12}, {\"USER_ID\": 2, \"amount\": 600, \"items\": 2},\n",
    "            {\"USER_ID\": 77, \"amount\": 60, \"items\": 12,\"device\":\"ios\"}, {\"USER_ID\": 23, \"amount\": 2, \"items\": 1,\"device\":\"android\"},\n",
    "            {\"USER_ID\": 3},{\"USER_ID\": 4,\"name\":\"Sagi\"},{\"amount\": 4,\"items\":50}]\n",
    "\n",
    "def update_hist(new_data_list):\n",
    "  global historical_data\n",
    "  for dic in new_data:\n",
    "        if all (k in dic for k in (\"USER_ID\",\"amount\",\"items\")): \n",
    "            if dic.get('USER_ID') in historical_data.keys():\n",
    "                  old_user_dic = historical_data.get(dic.get('USER_ID'))\n",
    "                  avg_old = old_user_dic.get('LTV_AVG')\n",
    "                  items_old = old_user_dic.get('LTV_ITEMS')\n",
    "                  amount = dic.get('amount')\n",
    "                  items_new = dic.get('items')\n",
    "                  up = ((avg_old * items_old) + amount)\n",
    "                  down = items_old + items_new\n",
    "                  update_avg = up / down\n",
    "                  update_items = items_old + items_new\n",
    "                  old_user_dic['LTV_AVG'] = update_avg\n",
    "                  old_user_dic['LTV_ITEMS'] = update_items\n",
    "                  historical_data[dic.get('USER_ID')] = old_user_dic\n",
    "            else :\n",
    "                  amount = dic.get('amount')\n",
    "                  items = dic.get('items')\n",
    "                  tmp = {\"LTV_AVG\": amount / items, \"LTV_ITEMS\": items}\n",
    "                  historical_data[dic.get('USER_ID')] = tmp\n",
    "             \n",
    "update_hist(new_data)\n",
    "print(historical_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fiverr question :\n",
    "# write script that will run query_1 for the last 30 days every day. you should able to change the number of days without change the script . if single day has an error for some reason the rest of the day still run ,the result should be sent to email as single csv file. the script should print only once to the screen the number of days with error . if there were no errors you should print \"no error in the last x days\" . \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# behalf python questions , create a function that:\n",
    "# 1.return a value of the given attr in the given dic\n",
    "# 2. check if a given string is a file path\n",
    "# 3.adds to given date N months\n",
    "# 4.return a value of a given attr (key) in a dic by its absolute path\n",
    "# 5.counts the number of accurances of a key in a dic\n",
    "# 6. return all distinct values of a given key in dic\n"
   ]
  }
 ]
}