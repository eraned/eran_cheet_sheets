{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.6 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "eran     3\nroni     6\nnoa      9\niris    12\ndtype: int64\n       Name  marks\nrank1   Tom     99\nrank2  Jack     98\nrank3  nick     95\nrank4  juli     90\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "obj = pd.Series([3,6,9,12],index=['eran','roni','noa','iris'])\n",
    "print(obj)\n",
    "\n",
    "\n",
    "data = {\"Name\":[\"Tom\", \"Jack\", \"nick\", \"juli\"], \"marks\":[99, 98, 95, 90]}\n",
    "df = pd.DataFrame(data, index =[\"rank1\", \"rank2\", \"rank3\", \"rank4\"])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Index([' Green', 'Black ', ' Red ', 'White', ' Pink '], dtype='object')\nIndex(['Green', 'Black', 'Red', 'White', 'Pink'], dtype='object')\nIndex(['Green', 'Black ', 'Red ', 'White', 'Pink '], dtype='object')\nIndex([' Green', 'Black', ' Red', 'White', ' Pink'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#  Write a Pandas program to remove whitespaces, left sided whitespaces and right sided whitespaces of the string values of a given pandas series\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "color1 = pd.Index([' Green', 'Black ', ' Red ', 'White', ' Pink '])\n",
    "print(color1)\n",
    "print(color1.str.strip())\n",
    "print(color1.str.lstrip())\n",
    "print(color1.str.rstrip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  student_id              name  marks\n0         S1  Danniella Fenton    200\n1         S2      Ryder Storey    210\n2         S3      Bryce Jensen    190\n3         S4         Ed Bernal    222\n4         S5       Kwame Morin    199\n0         S4  Scarlette Fisher    201\n1         S5  Carla Williamson    200\n2         S6       Dante Morse    198\n3         S7    Kaiser William    219\n4         S8   Madeeha Preston    201\n"
     ]
    }
   ],
   "source": [
    "# Write a Pandas program to join the two given dataframes along rows and assign all data.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "student_data1 = pd.DataFrame({\n",
    "        'student_id': ['S1', 'S2', 'S3', 'S4', 'S5'],\n",
    "         'name': ['Danniella Fenton', 'Ryder Storey', 'Bryce Jensen', 'Ed Bernal', 'Kwame Morin'], \n",
    "        'marks': [200, 210, 190, 222, 199]})\n",
    "\n",
    "student_data2 = pd.DataFrame({\n",
    "        'student_id': ['S4', 'S5', 'S6', 'S7', 'S8'],\n",
    "        'name': ['Scarlette Fisher', 'Carla Williamson', 'Dante Morse', 'Kaiser William', 'Madeeha Preston'], \n",
    "        'marks': [201, 200, 198, 219, 201]})\n",
    "\n",
    "result_data = pd.concat([student_data1, student_data2])\n",
    "print(result_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Original DataFrames:\n  student_id              name  marks\n0         S1  Danniella Fenton    200\n1         S2      Ryder Storey    210\n2         S3      Bryce Jensen    190\n3         S4         Ed Bernal    222\n4         S5       Kwame Morin    199\n\nNew Row(s)\nstudent_id                  S6\nname          Scarlette Fisher\nmarks                      205\ndtype: object\n\nCombined Data:\n  student_id              name  marks\n0         S1  Danniella Fenton    200\n1         S2      Ryder Storey    210\n2         S3      Bryce Jensen    190\n3         S4         Ed Bernal    222\n4         S5       Kwame Morin    199\n5         S6  Scarlette Fisher    205\n"
     ]
    }
   ],
   "source": [
    "# Write a Pandas program to append rows to an existing DataFrame and display the combined data.\n",
    "\n",
    "import pandas as pd\n",
    "student_data1 = pd.DataFrame({\n",
    "        'student_id': ['S1', 'S2', 'S3', 'S4', 'S5'],\n",
    "         'name': ['Danniella Fenton', 'Ryder Storey', 'Bryce Jensen', 'Ed Bernal', 'Kwame Morin'], \n",
    "        'marks': [200, 210, 190, 222, 199]})\n",
    "\n",
    "s6 = pd.Series(['S6', 'Scarlette Fisher', 205], index=['student_id', 'name', 'marks'])\n",
    "combined_data = student_data1.append(s6, ignore_index = True)\n",
    "print(combined_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  student_id              name  marks\n0         S1  Danniella Fenton    200\n1         S2      Ryder Storey    210\n2         S3      Bryce Jensen    190\n3         S4         Ed Bernal    222\n4         S5       Kwame Morin    199\n0         S4  Scarlette Fisher    201\n1         S5  Carla Williamson    200\n2         S6       Dante Morse    198\n3         S7    Kaiser William    219\n4         S8   Madeeha Preston    201\n\nNow join the said result_data and df_exam_data along student_id:\n  student_id              name  marks  exam_id\n0         S1  Danniella Fenton    200       23\n1         S2      Ryder Storey    210       45\n2         S3      Bryce Jensen    190       12\n3         S4         Ed Bernal    222       67\n4         S4  Scarlette Fisher    201       67\n5         S5       Kwame Morin    199       21\n6         S5  Carla Williamson    200       21\n7         S7    Kaiser William    219       55\n8         S8   Madeeha Preston    201       33\n"
     ]
    }
   ],
   "source": [
    "# Write a Pandas program to join the two given dataframes along rows and merge with another dataframe along the common column id.\n",
    "import pandas as pd\n",
    "student_data1 = pd.DataFrame({\n",
    "        'student_id': ['S1', 'S2', 'S3', 'S4', 'S5'],\n",
    "         'name': ['Danniella Fenton', 'Ryder Storey', 'Bryce Jensen', 'Ed Bernal', 'Kwame Morin'], \n",
    "        'marks': [200, 210, 190, 222, 199]})\n",
    "\n",
    "student_data2 = pd.DataFrame({\n",
    "        'student_id': ['S4', 'S5', 'S6', 'S7', 'S8'],\n",
    "        'name': ['Scarlette Fisher', 'Carla Williamson', 'Dante Morse', 'Kaiser William', 'Madeeha Preston'], \n",
    "        'marks': [201, 200, 198, 219, 201]})\n",
    "\n",
    "exam_data = pd.DataFrame({\n",
    "        'student_id': ['S1', 'S2', 'S3', 'S4', 'S5', 'S7', 'S8', 'S9', 'S10', 'S11', 'S12', 'S13'],\n",
    "        'exam_id': [23, 45, 12, 67, 21, 55, 33, 14, 56, 83, 88, 12]})\n",
    "\n",
    "result_data = pd.concat([student_data1, student_data2])\n",
    "print(result_data)\n",
    "final_merged_data = pd.merge(result_data, exam_data, on='student_id')\n",
    "print(final_merged_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  key1 key2    P    Q    R    S\n0   K0   K0   P0   Q0  NaN  NaN\n1   K0   K1   P1   Q1  NaN  NaN\n2   K1   K0   P2   Q2  NaN  NaN\n3   K2   K1   P3   Q3  NaN  NaN\n4   K0   K0  NaN  NaN   R0   S0\n5   K1   K0  NaN  NaN   R1   S1\n6   K1   K0  NaN  NaN   R2   S2\n7   K2   K0  NaN  NaN   R3   S3\n"
     ]
    }
   ],
   "source": [
    "# Write a Pandas program to merge two given dataframes with different columns.\n",
    "\n",
    "import pandas as pd\n",
    "data1 = pd.DataFrame({'key1': ['K0', 'K0', 'K1', 'K2'],\n",
    "                     'key2': ['K0', 'K1', 'K0', 'K1'],\n",
    "                     'P': ['P0', 'P1', 'P2', 'P3'],\n",
    "                     'Q': ['Q0', 'Q1', 'Q2', 'Q3']}) \n",
    "data2 = pd.DataFrame({'key1': ['K0', 'K1', 'K1', 'K2'],\n",
    "                      'key2': ['K0', 'K0', 'K0', 'K0'],\n",
    "                      'R': ['R0', 'R1', 'R2', 'R3'],\n",
    "                      'S': ['S0', 'S1', 'S2', 'S3']})\n",
    "\n",
    "result = pd.concat([data1,data2], axis=0, ignore_index=True)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "True\nFalse\n"
     ]
    }
   ],
   "source": [
    "# Write a Python program to check that a string contains only a certain set of characters (in this case a-z, A-Z and 0-9).\n",
    "import re\n",
    "def is_allowed_specific_char(string):\n",
    "    charRe = re.compile(r'[^a-zA-Z0-9.]')\n",
    "    string = charRe.search(string)\n",
    "    return not bool(string)\n",
    "\n",
    "print(is_allowed_specific_char(\"ABCDEFabcdef123450\")) \n",
    "print(is_allowed_specific_char(\"*&%@#!}{\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1, 2, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "# Write a Python function that takes a list and returns a new list with unique elements of the first list.\n",
    "def unique_list(l):\n",
    "  x = []\n",
    "  for a in l:\n",
    "    if a not in x:\n",
    "      x.append(a)\n",
    "  return x\n",
    "\n",
    "print(unique_list([1,2,3,3,3,3,4,5])) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "yah\n\n"
     ]
    }
   ],
   "source": [
    "# Write a Python program that accepts a word from the user and reverse it.\n",
    "\n",
    "word = input(\"Input a word to reverse: \")\n",
    "\n",
    "for char in range(len(word) - 1, -1, -1):\n",
    "  print(word[char], end=\"\")\n",
    "print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1\n1\n2\n3\n5\n8\n13\n21\n34\n"
     ]
    }
   ],
   "source": [
    "# Write a Python program to get the Fibonacci series between 0 to 50\n",
    "\n",
    "x,y=0,1\n",
    "\n",
    "while y<50:\n",
    "    print(y)\n",
    "    x,y = y,x+y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a Pandas program to display the first, last name, salary and department number for those employees whose first name does not contain the letter 'M'.\n",
    "import pandas as pd\n",
    "employees = pd.read_csv(r\"EMPLOYEES.csv\")\n",
    "departments = pd.read_csv(r\"DEPARTMENTS.csv\")\n",
    "job_history = pd.read_csv(r\"JOB_HISTORY.csv\")\n",
    "jobs = pd.read_csv(r\"JOBS.csv\")\n",
    "countries = pd.read_csv(r\"COUNTRIES.csv\")\n",
    "regions = pd.read_csv(r\"REGIONS.csv\")\n",
    "locations = pd.read_csv(r\"LOCATIONS.csv\")\n",
    "print(\"Last name       First name      Salary    Department ID\")\n",
    "\n",
    "result = employees[employees['first_name'].str.find('M')==-1]\n",
    "for index, row in result.iterrows():\n",
    "    print(row['last_name'].ljust(15),row['first_name'].ljust(15),str(row['salary']).ljust(9),row['department_id'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Original DataFrame:\n   school_code class            name date_Of_Birth   age  height  weight  \\\nS1        s001     V  Alberto Franco     15/05/2002   12     173      35   \nS2        s002     V    Gino Mcneill     17/05/2002   12     192      32   \nS3        s003    VI     Ryan Parkes     16/02/1999   13     186      33   \nS4        s001    VI    Eesha Hinton     25/09/1998   13     167      30   \nS5        s002     V    Gino Mcneill     11/05/2002   14     151      31   \nS6        s004    VI    David Parkes     15/09/1997   12     159      32   \n\n    address  \nS1  street1  \nS2  street2  \nS3  street3  \nS4  street1  \nS5  street2  \nS6  street4  \n\nSplit the said data on school_code wise:\n\nGroup:\ns001\n   school_code class            name date_Of_Birth   age  height  weight  \\\nS1        s001     V  Alberto Franco     15/05/2002   12     173      35   \nS4        s001    VI    Eesha Hinton     25/09/1998   13     167      30   \n\n    address  \nS1  street1  \nS4  street1  \n\nGroup:\ns002\n   school_code class          name date_Of_Birth   age  height  weight  \\\nS2        s002     V  Gino Mcneill     17/05/2002   12     192      32   \nS5        s002     V  Gino Mcneill     11/05/2002   14     151      31   \n\n    address  \nS2  street2  \nS5  street2  \n\nGroup:\ns003\n   school_code class         name date_Of_Birth   age  height  weight  address\nS3        s003    VI  Ryan Parkes     16/02/1999   13     186      33  street3\n\nGroup:\ns004\n   school_code class          name date_Of_Birth   age  height  weight  \\\nS6        s004    VI  David Parkes     15/09/1997   12     159      32   \n\n    address  \nS6  street4  \n\nType of the object:\n<class 'pandas.core.groupby.generic.DataFrameGroupBy'>\n"
     ]
    }
   ],
   "source": [
    "# Write a Pandas program to split the following dataframe into groups based on school code. Also check the type of GroupBy object.\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', None)\n",
    "#pd.set_option('display.max_columns', None)\n",
    "student_data = pd.DataFrame({\n",
    "    'school_code': ['s001','s002','s003','s001','s002','s004'],\n",
    "    'class': ['V', 'V', 'VI', 'VI', 'V', 'VI'],\n",
    "    'name': ['Alberto Franco','Gino Mcneill','Ryan Parkes', 'Eesha Hinton', 'Gino Mcneill', 'David Parkes'],\n",
    "    'date_Of_Birth ': ['15/05/2002','17/05/2002','16/02/1999','25/09/1998','11/05/2002','15/09/1997'],\n",
    "    'age': [12, 12, 13, 13, 14, 12],\n",
    "    'height': [173, 192, 186, 167, 151, 159],\n",
    "    'weight': [35, 32, 33, 30, 31, 32],\n",
    "    'address': ['street1', 'street2', 'street3', 'street1', 'street2', 'street4']},\n",
    "    index=['S1', 'S2', 'S3', 'S4', 'S5', 'S6'])\n",
    "\n",
    "print(\"Original DataFrame:\")\n",
    "print(student_data)\n",
    "print('\\nSplit the said data on school_code wise:')\n",
    "result = student_data.groupby(['school_code'])\n",
    "for name,group in result:\n",
    "    print(\"\\nGroup:\")\n",
    "    print(name)\n",
    "    print(group)\n",
    "print(\"\\nType of the object:\")\n",
    "print(type(result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A valid UID must follow the rules below:\n",
    "# It must contain at least  uppercase English alphabet characters.\n",
    "# It must contain at least  digits ( - ).\n",
    "# It should only contain alphanumeric characters ( - ,  -  &  - ).\n",
    "# No character should repeat.\n",
    "# There must be exactly  characters in a valid UID.\n",
    "\n",
    "import re\n",
    "\n",
    "for _ in range(int(input())):\n",
    "    u = ''.join(sorted(input()))\n",
    "    try:\n",
    "        assert re.search(r'[A-Z]{2}', u)\n",
    "        assert re.search(r'\\d\\d\\d', u)\n",
    "        assert not re.search(r'[^a-zA-Z0-9]', u)\n",
    "        assert not re.search(r'(.)\\1', u)\n",
    "        assert len(u) == 10\n",
    "    except:\n",
    "        print('Invalid')\n",
    "    else:\n",
    "        print('Valid')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'fdvf'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-586722d3f2f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0memails\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: 'fdvf'"
     ]
    }
   ],
   "source": [
    "# Valid email addresses must follow these rules:\n",
    "# It must have the username@websitename.extension format type.\n",
    "# The username can only contain letters, digits, dashes and underscores.\n",
    "# The website name can only have letters and digits.\n",
    "# The maximum length of the extension is .\n",
    "\n",
    "import re\n",
    "\n",
    "def fun(s):\n",
    "  pattern = re.compile(\"^[\\\\w-]+@[0-9a-zA-Z]+\\\\.[a-z]{1,3}$\")\n",
    "  return pattern.match(s)\n",
    "       \n",
    "def filter_mail(emails):\n",
    "    return list(filter(fun, emails))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    n = int(input())\n",
    "    emails = []\n",
    "    for _ in range(n):\n",
    "        emails.append(input())\n",
    "\n",
    "filtered_emails = filter_mail(emails)\n",
    "filtered_emails.sort()\n",
    "print(filtered_emails)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{1: {'LTV_AVG': 8.956521739130435, 'LTV_ITEMS': 115}, 7: {'LTV_AVG': 4.5, 'LTV_ITEMS': 2}, 2: {'LTV_AVG': 203.33333333333334, 'LTV_ITEMS': 3}, 3: {'LTV_AVG': 5.0, 'LTV_ITEMS': 12}, 77: {'LTV_AVG': 5.0, 'LTV_ITEMS': 12}, 23: {'LTV_AVG': 2.0, 'LTV_ITEMS': 1}}\n"
     ]
    }
   ],
   "source": [
    "# optimove question :\n",
    "# We Have a dictionary with LTV data per user “historical_data”.\n",
    "\n",
    "# Every few minutes we are receiving a smaller list with activities of new/existing users “new_data”.\n",
    "# Complete the function update_hist that recieves a list of events as an input and updates the historical_data dictionary\n",
    "\n",
    "\n",
    "historical_data = {1: {\"LTV_AVG\": 10, \"LTV_ITEMS\": 100}, 7: {\"LTV_AVG\": 4.5, \"LTV_ITEMS\": 2}}\n",
    "\n",
    "new_data = [{\"USER_ID\": 1, \"amount\": 30, \"items\": 15}, {\"USER_ID\": 2, \"amount\": 10, \"items\": 1},\n",
    "            {\"USER_ID\": 3, \"amount\": 60, \"items\": 12}, {\"USER_ID\": 2, \"amount\": 600, \"items\": 2},\n",
    "            {\"USER_ID\": 77, \"amount\": 60, \"items\": 12,\"device\":\"ios\"}, {\"USER_ID\": 23, \"amount\": 2, \"items\": 1,\"device\":\"android\"},\n",
    "            {\"USER_ID\": 3},{\"USER_ID\": 4,\"name\":\"Sagi\"},{\"amount\": 4,\"items\":50}]\n",
    "\n",
    "def update_hist(new_data_list):\n",
    "  global historical_data\n",
    "  for dic in new_data:\n",
    "        if all (k in dic for k in (\"USER_ID\",\"amount\",\"items\")): \n",
    "            if dic.get('USER_ID') in historical_data.keys():\n",
    "                  old_user_dic = historical_data.get(dic.get('USER_ID'))\n",
    "                  avg_old = old_user_dic.get('LTV_AVG')\n",
    "                  items_old = old_user_dic.get('LTV_ITEMS')\n",
    "                  amount = dic.get('amount')\n",
    "                  items_new = dic.get('items')\n",
    "                  up = ((avg_old * items_old) + amount)\n",
    "                  down = items_old + items_new\n",
    "                  update_avg = up / down\n",
    "                  update_items = items_old + items_new\n",
    "                  old_user_dic['LTV_AVG'] = update_avg\n",
    "                  old_user_dic['LTV_ITEMS'] = update_items\n",
    "                  historical_data[dic.get('USER_ID')] = old_user_dic\n",
    "            else :\n",
    "                  amount = dic.get('amount')\n",
    "                  items = dic.get('items')\n",
    "                  tmp = {\"LTV_AVG\": amount / items, \"LTV_ITEMS\": items}\n",
    "                  historical_data[dic.get('USER_ID')] = tmp\n",
    "             \n",
    "update_hist(new_data)\n",
    "print(historical_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " SELECT id FROM  SPL WHERE date =  2020-11-16 \n SELECT id FROM  SPL WHERE date =  2020-11-17 \n SELECT id FROM  SPL WHERE date =  2020-11-18 \n SELECT id FROM  SPL WHERE date =  2020-11-19 \n SELECT id FROM  SPL WHERE date =  2020-11-20 \n SELECT id FROM  SPL WHERE date =  2020-11-21 \n SELECT id FROM  SPL WHERE date =  2020-11-22 \n SELECT id FROM  SPL WHERE date =  2020-11-23 \n SELECT id FROM  SPL WHERE date =  2020-11-24 \n SELECT id FROM  SPL WHERE date =  2020-11-25 \n SELECT id FROM  SPL WHERE date =  2020-11-26 \n SELECT id FROM  SPL WHERE date =  2020-11-27 \n SELECT id FROM  SPL WHERE date =  2020-11-28 \n SELECT id FROM  SPL WHERE date =  2020-11-29 \n SELECT id FROM  SPL WHERE date =  2020-11-30 \n SELECT id FROM  SPL WHERE date =  2020-12-01 \n SELECT id FROM  SPL WHERE date =  2020-12-02 \n SELECT id FROM  SPL WHERE date =  2020-12-03 \n SELECT id FROM  SPL WHERE date =  2020-12-04 \n SELECT id FROM  SPL WHERE date =  2020-12-05 \n SELECT id FROM  SPL WHERE date =  2020-12-06 \n SELECT id FROM  SPL WHERE date =  2020-12-07 \n SELECT id FROM  SPL WHERE date =  2020-12-08 \n SELECT id FROM  SPL WHERE date =  2020-12-09 \n SELECT id FROM  SPL WHERE date =  2020-12-10 \n SELECT id FROM  SPL WHERE date =  2020-12-11 \n SELECT id FROM  SPL WHERE date =  2020-12-12 \n SELECT id FROM  SPL WHERE date =  2020-12-13 \n SELECT id FROM  SPL WHERE date =  2020-12-14 \n SELECT id FROM  SPL WHERE date =  2020-12-15 \n"
     ]
    }
   ],
   "source": [
    "# fiverr question :\n",
    "# write script that will run query_1 for the last 30 days every day. you should able to change the number of days without change the script . if single day has an error for some reason the rest of the day still run ,the result should be sent to email as single csv file. the script should print only once to the screen the number of days with error . if there were no errors you should print \"no error in the last x days\" . \n",
    "\n",
    "from datetime import timedelta,date\n",
    "\n",
    "error_ctr = 0\n",
    "start_date = date.today() - timedelta(30)\n",
    "end_date = date.today()\n",
    "\n",
    "def daterange(start_date, end_date):\n",
    "    for n in range(int((end_date - start_date).days)):\n",
    "        yield start_date + timedelta(n)\n",
    "\n",
    "for single_date in daterange(start_date, end_date):\n",
    "    try:\n",
    "        query = \" SELECT id FROM  SPL WHERE date =  {} \".format(single_date)\n",
    "        df = run_query(query)\n",
    "        df.to_csv('path to save')\n",
    "        df.to_json('path to save')\n",
    "        send_email(recipients , csv_path)\n",
    "        print(query)\n",
    "        # print(single_date.strftime(\"%Y%m%d\"))\n",
    "\n",
    "\n",
    "    except:\n",
    "        error_ctr += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'sports', 'music', 'news', 'movies'}\n"
     ]
    }
   ],
   "source": [
    "# behalf python questions , create a function that:\n",
    "# 1.return a value of the given attr in the given dic\n",
    "# 2. check if a given string is a file path\n",
    "# 3.adds to given date N months\n",
    "# from datetime import datetime, timedelta\n",
    "# from dateutil.relativedelta import *\n",
    "\n",
    "# date = datetime.now()\n",
    "# print(date)\n",
    "# date = date + relativedelta(months=+6)\n",
    "# print(date)\n",
    "\n",
    "# 4.return a value of a given attr (key) in a dic by its absolute path\n",
    "# 5.counts the number of accurances of a key in a list of dictionaries \n",
    "# from collections import Counter\n",
    "\n",
    "# data = {1: {\"LTV_AVG\": 10, \"LTV_ITEMS\": 100}, 7: {\"LTV_AVG\": 4.5, \"LTV_ITEMS\": 2}}\n",
    " \n",
    "# signs = Counter(k[0] for k in data if k.get(0))\n",
    "# for sign, count in signs.most_common():\n",
    "#      print(sign, count)\n",
    "\n",
    "\n",
    "# 6. return all distinct values of a given key in dic\n",
    "\n",
    "lis = [{\"abc\":\"movies\"}, {\"abc\": \"sports\"}, {\"abc\": \"music\"}, {\"xyz\": \"music\"}, {\"pqr\":\"music\"}, {\"pqr\":\"movies\"},{\"pqr\":\"sports\"}, {\"pqr\":\"news\"}, {\"pqr\":\"sports\"}]\n",
    "s = set( val for dic in lis for val in dic.values())\n",
    "print(s )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Value of y: {'a': {'b': 'c', 'd': 'e'}}\na\nb\nd\n{'a_b': 'c', 'a_d': 'e'}\n"
     ]
    }
   ],
   "source": [
    "# In python code, given a json object with nested objects , write a function that flattens all the objects to a single key value dictionary. Do not use the lib that actually performs this function. { a:{b:c,d:e} } becomes {a_b:c, a_d:e} ( not, a:\"b:c,d:e\" }\n",
    "\n",
    "\n",
    "unflat_json = {'a':{'b':'c','d':'e'}}\n",
    "\n",
    "def flatten_json(y):\n",
    "    out = {}\n",
    "    print('Value of y:',y)\n",
    "    def flatten(y, name =''):\n",
    "        if type(y) is dict:\n",
    "            for x in y:\n",
    "                print(x)\n",
    "                flatten(y[x], name + x + '_')\n",
    "        else:\n",
    "            out[name[:-1]] = y\n",
    "\n",
    "    flatten(y)\n",
    "    return out\n",
    "\n",
    "print(flatten_json(unflat_json))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perion data engineer python question :\n",
    "\n",
    "import json\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import csv\n",
    "import re\n",
    "import os\n",
    "import schedule\n",
    "import findspark\n",
    "import pyspark\n",
    "# import spark\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import ArrayType, StructField, StructType, StringType, IntegerType, DoubleType, DateType\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "\n",
    "\n",
    "def etl():\n",
    "\n",
    "    print(\"etl has started....\")\n",
    "    dirin = r\"/Users/eranedri/Desktop/perion_test/raw_data\"\n",
    "    dirout = r\"/Users/eranedri/Desktop/perion_test/data_out\"\n",
    "    historical_data = {}\n",
    "\n",
    "    for file in os.listdir(dirin):\n",
    "        print(\"loading converted json file to daraframe .... \")\n",
    "        with open(os.path.join(dirin, file), 'r', encoding='utf8') as f:\n",
    "            for line in f:\n",
    "                tmp = json.loads(line)\n",
    "                if tmp['keyword'] in historical_data.keys():\n",
    "                  old_keyword_dic = historical_data.get(tmp['keyword'])\n",
    "                  avg_old_searches = old_keyword_dic.get(\n",
    "                      'avg_searches_monthly_volume')\n",
    "                  avg_old_cpc = old_keyword_dic.get('avg_cpc')\n",
    "                  up_seraches = (avg_old_searches + tmp['monthly_volume'])\n",
    "                  up_cpc = (avg_old_cpc + tmp['cpc'])\n",
    "                  update_avg_searches = up_seraches / 2\n",
    "                  update_avg_cpc = up_cpc / 2\n",
    "                  old_keyword_dic['avg_searches_monthly_volume'] = update_avg_searches\n",
    "                  old_keyword_dic['avg_cpc'] = update_avg_cpc\n",
    "                  historical_data[tmp['keyword']] = old_keyword_dic\n",
    "                else:\n",
    "                    keyword_dic = {\n",
    "                        \"avg_searches_monthly_volume\": tmp['monthly_volume'], \"avg_cpc\": tmp['cpc'], \"dt\": tmp['dt']}\n",
    "                    historical_data[tmp['keyword']] = keyword_dic\n",
    "\n",
    "        with open(os.path.join(dirout, file), 'w', encoding='utf8') as f:\n",
    "            print(\"writing file to a parquet file .... \")\n",
    "            df = spark.createDataFrame(historical_data, schema)\n",
    "            df.write.parquet(f)\n",
    "\n",
    "\n",
    "def some_job():\n",
    "    etl()\n",
    "    print(\"schedule is running...\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    findspark.init()\n",
    "    sc = SparkContext.getOrCreate()\n",
    "    sqlContext = SQLContext(sc)\n",
    "    spark = SparkSession(sc)\n",
    "\n",
    "    schema = StructType([\n",
    "        StructField('keyword', StringType(), True),\n",
    "        StructField('avg_searches_monthly_volume', DoubleType(), True),\n",
    "        StructField('avg_cpc', DoubleType(), True),\n",
    "        StructField('dt', StringType(), True)\n",
    "    ])\n",
    "\n",
    "    # etl()\n",
    "\n",
    "  schedule.every(10).minutes.do(some_job)\n",
    "  while True:\n",
    "    schedule.run_pending()\n",
    "    time.sleep(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:57951)\nTraceback (most recent call last):\n  File \"/usr/local/Cellar/apache-spark/3.0.1/libexec/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 977, in _get_connection\n    connection = self.deque.pop()\nIndexError: pop from an empty deque\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/Cellar/apache-spark/3.0.1/libexec/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1115, in start\n    self.socket.connect((self.address, self.port))\nConnectionRefusedError: [Errno 61] Connection refused\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "Py4JNetworkError",
     "evalue": "An error occurred while trying to connect to the Java server (127.0.0.1:57951)",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/3.0.1/libexec/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m_get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    976\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 977\u001b[0;31m             \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeque\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    978\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: pop from an empty deque",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/3.0.1/libexec/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1114\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1115\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1116\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakefile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 61] Connection refused",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mPy4JNetworkError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-0a9b32ded393>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0msqlContext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSQLContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m     \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/3.0.1/libexec/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sparkContext, jsparkSession)\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mjsparkSession\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetDefaultSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misDefined\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m                     \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetDefaultSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m                         \u001b[0;34m.\u001b[0m\u001b[0msparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misStopped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/3.0.1/libexec/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1690\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mUserHelpAutoCompletion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1692\u001b[0;31m         answer = self._gateway_client.send_command(\n\u001b[0m\u001b[1;32m   1693\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mREFLECTION_COMMAND_NAME\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1694\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mREFL_GET_UNKNOWN_SUB_COMMAND_NAME\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_id\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/3.0.1/libexec/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1029\u001b[0m          \u001b[0;32mif\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m         \"\"\"\n\u001b[0;32m-> 1031\u001b[0;31m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1032\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/3.0.1/libexec/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m_get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    977\u001b[0m             \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeque\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    978\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 979\u001b[0;31m             \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    980\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/3.0.1/libexec/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m_create_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    983\u001b[0m         connection = GatewayConnection(\n\u001b[1;32m    984\u001b[0m             self.gateway_parameters, self.gateway_property)\n\u001b[0;32m--> 985\u001b[0;31m         \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/3.0.1/libexec/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1125\u001b[0m                 \u001b[0;34m\"server ({0}:{1})\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1127\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mPy4JNetworkError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1129\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_authenticate_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JNetworkError\u001b[0m: An error occurred while trying to connect to the Java server (127.0.0.1:57951)"
     ]
    }
   ],
   "source": [
    "# bank hapoalim python test : \n",
    "\n",
    "import json\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import csv\n",
    "import re\n",
    "import os\n",
    "import schedule\n",
    "import findspark\n",
    "import pyspark\n",
    "from pyspark.ml.classification import LogisticRegression, DecisionTreeClassifier, GBTClassifier, LinearSVC , RandomForestClassifier\n",
    "from pyspark.ml.feature import StringIndexer , VectorAssembler\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder ,CrossValidator ,CrossValidatorModel\n",
    "from pyspark.ml import pipeline\n",
    "# import spark\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import ArrayType, StructField, StructType, StringType, IntegerType, DoubleType, DateType\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def job():\n",
    "\n",
    "    print(\"job has started....\")\n",
    "    print(\"calculating results.... \")\n",
    "\n",
    "    # a\n",
    "    print(\"a.... \")\n",
    "    sqlDF = spark.sql(\"SELECT credit_history FROM table GROUP BY 1\")\n",
    "    sqlDF.show()\n",
    "\n",
    "    # b\n",
    "    print(\"b.... \")\n",
    "    sqlDF = spark.sql(\"SELECT credit_amount , personal_status , age , employment  FROM table where credit_amount > 100000 AND age > 30 AND personal_status IN ('male div/sep','male single','male mar/wid') AND employment IN ('unemployed')\")\n",
    "    sqlDF.show()\n",
    "\n",
    "    # c\n",
    "    print(\"c.... \")\n",
    "    sqlDF = spark.sql(\"SELECT purpose , AVG(credit_amount) FROM table GROUP BY 1 ORDER BY 2 DESC\")\n",
    "    sqlDF.show()\n",
    "\n",
    "    # d\n",
    "    print(\"d.... \")\n",
    "    splitDF = df.withColumn(\"employment\",F.split(F.regexp_replace(\"employment\", \"([=><X])(?!$)\", r\"$1,\"), \",\"))\n",
    "    splitDF.show()\n",
    "    print(sqlDF.count())\n",
    "\n",
    "\n",
    "\n",
    "    # df = df.withColumn(\"employment\", df[\"employment\"].cast(IntegerType())).show()\n",
    "    # splitDF = df.select(F.split(df.employment, '[=><X123]').alias('array'))\n",
    "    # splitDF.show()\n",
    "\n",
    "    # sqlDF = spark.sql(\"SELECT SPLIT(employment,'[=><X1234]') as array FROM table where employment <> 'unemployed'\")\n",
    "    # sqlDF.show()\n",
    "    # print(sqlDF.count())\n",
    "\n",
    "\n",
    "def build_model():\n",
    "    print(\"building model....\")\n",
    "    # print(df.describe().toPandas())\n",
    "    dataset = df.select(F.col('age').cast('integer'),F.col('existing_credits').cast('integer'),df.employment,df.savings_status,df.other_payment_plans , df.housing,df.property_magnitude,df['class'])\n",
    "    dataset = StringIndexer(inputCol='employment', outputCol='employment_encoded', handleInvalid='keep').fit(dataset).transform(dataset)\n",
    "    dataset = StringIndexer(inputCol='savings_status', outputCol='savings_status_encoded', handleInvalid='keep').fit(dataset).transform(dataset)\n",
    "    dataset = StringIndexer(inputCol='other_payment_plans', outputCol='other_payment_plans_encoded', handleInvalid='keep').fit(dataset).transform(dataset)\n",
    "    dataset = StringIndexer(inputCol='housing', outputCol='housing_encoded', handleInvalid='keep').fit(dataset).transform(dataset)\n",
    "    dataset = StringIndexer(inputCol='property_magnitude', outputCol='property_magnitude_encoded', handleInvalid='keep').fit(dataset).transform(dataset)\n",
    "    dataset = StringIndexer(inputCol='class', outputCol='class_encoded', handleInvalid='keep').fit(dataset).transform(dataset)\n",
    "    dataset.show()\n",
    "\n",
    "    required_features = ['age','existing_credits','employment_encoded','savings_status_encoded','other_payment_plans_encoded','housing_encoded','property_magnitude_encoded']\n",
    "    assembler = VectorAssembler(inputCols=required_features, outputCol='features')\n",
    "    transformed_data = assembler.transform(dataset)\n",
    "    transformed_data.show()\n",
    "    (training_data, test_data) = transformed_data.randomSplit([0.8,0.2])\n",
    "\n",
    "    lr = LogisticRegression(labelCol= 'class_encoded' , featuresCol='features', maxIter=1000,regParam=0.02,elasticNetParam=0.8)\n",
    "    dtc = DecisionTreeClassifier(labelCol= 'class_encoded' , featuresCol='features',maxDepth = 3)\n",
    "    rfc = RandomForestClassifier(labelCol= 'class_encoded' , featuresCol='features')\n",
    "    gbt = GBTClassifier(maxIter=100)\n",
    "\n",
    "    def evaluate(model_name,train,test):\n",
    "        evaluator = MulticlassClassificationEvaluator(labelCol='class_encoded', predictionCol='prediction', metricName='accuracy')\n",
    "        paramGrid = ParamGridBuilder().build()\n",
    "        crossval = CrossValidator(estimator=model_name,\\\n",
    "        evaluator=evaluator, \\\n",
    "        estimatorParamMaps=paramGrid,\\\n",
    "        numFolds=3)\n",
    "        cvModel = crossval.fit(train)\n",
    "        cvModel_metrics = cvModel.avgMetrics\n",
    "        transformed_data = cvModel.transform(test)\n",
    "        test_metrics = evaluator.evaluate(transformed_data)\n",
    "        return (cvModel_metrics, test_metrics)\n",
    "\n",
    "    model_names = [lr,dtc,rfc,gbt]\n",
    "\n",
    "    for model in model_names:\n",
    "        a = evaluate(model,training_data,test_data)\n",
    "        print(model,a)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    findspark.init()\n",
    "    sc = SparkContext.getOrCreate()\n",
    "    sqlContext = SQLContext(sc)\n",
    "    spark = SparkSession(sc)\n",
    "\n",
    "\n",
    "    dirin = r\"/Users/eranedri/Desktop/bank_ass/dataset_31_credit-g.csv\"\n",
    "    print(\"loading csv file to spark daraframe .... \")\n",
    "    df = spark.read.format(\"csv\").option(\"header\", \"true\").load(dirin)\n",
    "    df.createOrReplaceTempView(\"table\")\n",
    "    # job()\n",
    "    build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}